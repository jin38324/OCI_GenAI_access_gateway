{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoint Feature Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xai.grok-4-fast-non-reasoning\n",
      "xai.grok-4-fast-reasoning\n",
      "openai.gpt-oss-120b\n",
      "openai.gpt-oss-20b\n",
      "meta.llama-guard-4-12b\n",
      "xai.grok-4\n",
      "cohere.command-latest\n",
      "cohere.command-plus-latest\n",
      "cohere.embed-v4.0\n",
      "xai.grok-3-fast\n",
      "xai.grok-3\n",
      "xai.grok-3-mini-fast\n",
      "xai.grok-3-mini\n",
      "cohere.rerank-v3.5\n",
      "cohere.embed-english-image-v3.0\n",
      "meta.llama-4-maverick-17b-128e-instruct-fp8\n",
      "cohere.embed-english-light-image-v3.0\n",
      "content-moderator\n",
      "cohere.embed-multilingual-light-image-v3.0\n",
      "meta.llama-4-scout-17b-16e-instruct\n",
      "cohere.embed-multilingual-image-v3.0\n",
      "cohere.command-a-03-2025\n",
      "meta.llama-3.3-70b-instruct\n",
      "protectai.deberta-v3-base-prompt-injection-v2\n",
      "urchade.gliner-large-v2-1\n",
      "cohere.rerank-multilingual-v3.1\n",
      "cohere.command-r-08-2024\n",
      "meta.llama-3.2-11b-vision-instruct\n",
      "meta.llama-3.2-90b-vision-instruct\n",
      "cohere.command-r-plus-08-2024\n",
      "meta.llama-3.1-70b-instruct\n",
      "meta.llama-3.1-405b-instruct\n",
      "cohere.command-r-16k\n",
      "cohere.command-r-plus\n",
      "cohere.rerank-english-v3.1\n",
      "meta.llama-3-70b-instruct\n",
      "cohere.command\n",
      "cohere.command-light\n",
      "cohere.embed-english-light-v3.0\n",
      "cohere.embed-english-v3.0\n",
      "cohere.embed-multilingual-light-v3.0\n",
      "cohere.embed-multilingual-v3.0\n",
      "cohere.embed-english-light-v2.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"ocigenerativeai\", base_url=\"http://127.0.0.1:8088/v1/\", max_retries=0\n",
    ")\n",
    "models = client.models.list()\n",
    "\n",
    "# Test chat completions\n",
    "for model in models:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with non-streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = [\n",
    "    \"xai.grok-4-fast-non-reasoning\",\n",
    "    # \"openai.gpt-4o\",\n",
    "    # \"cohere.command-latest\",\n",
    "    \"cohere.command-a-03-2025\",\n",
    "    # \"meta.llama-latest\"\n",
    "    \"xai.grok-3-mini\",\n",
    "    \"meta.llama-4-maverick-17b-128e-instruct-fp8\",\n",
    "    \"openai.gpt-oss-20b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xai.grok-4-fast-non-reasoning  :  Hello! ‰Ω†Â•ΩÔºÅ How can I help you today?\n",
      "\n",
      "prompt_tokens: 35 \n",
      "completion_tokens: 12 \n",
      "total_tokens: 48\n",
      "reasoning_tokens: 1\n",
      "cohere.command-a-03-2025  :  Hello! ‰Ω†Â•ΩÔºÅ It's great to meet you. How can I assist you today? Whether it's answering questions, helping with a task, or just having a chat, I'm here for you.\n",
      "\n",
      "prompt_tokens: 6 \n",
      "completion_tokens: 43 \n",
      "total_tokens: 49\n",
      "xai.grok-3-mini  :  Hello! ‰Ω†Â•ΩÔºÅ  \n",
      "\n",
      "It's great to hear from you. I'm Grok, an AI created by xAI to be helpful and truthful. What can I assist you with today? üòä\n",
      "\n",
      "prompt_tokens: 12 \n",
      "completion_tokens: 39 \n",
      "total_tokens: 348\n",
      "reasoning_tokens: 297\n",
      "meta.llama-4-maverick-17b-128e-instruct-fp8  :  Hello! ‰Ω†Â•Ω! It's nice to meet you! Is there something I can help you with or would you like to chat?\n",
      "\n",
      "prompt_tokens: 16 \n",
      "completion_tokens: 27 \n",
      "total_tokens: 43\n",
      "openai.gpt-oss-20b  :  Hello! ‰Ω†Â•ΩÔºÅüåü  \n",
      "How can I help you today?\n",
      "\n",
      "prompt_tokens: 72 \n",
      "completion_tokens: 54 \n",
      "total_tokens: 126\n"
     ]
    }
   ],
   "source": [
    "# Test chat completions with non-streaming response\n",
    "\n",
    "for model_name in test_models:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello! ‰Ω†Â•ΩÔºÅ\"}],\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    print(model_name, \" : \", completion.choices[0].message.content)\n",
    "    print(\n",
    "        \"\\nprompt_tokens:\",\n",
    "        completion.usage.prompt_tokens,\n",
    "        \"\\ncompletion_tokens:\",\n",
    "        completion.usage.completion_tokens,\n",
    "        \"\\ntotal_tokens:\",\n",
    "        completion.usage.total_tokens,\n",
    "    )\n",
    "    if completion.usage.completion_tokens_details:\n",
    "        print(\n",
    "            \"reasoning_tokens:\",\n",
    "            completion.usage.completion_tokens_details.reasoning_tokens,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test chat completions with streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " xai.grok-4-fast-non-reasoning  : Hello! ‰Ω†Â•ΩÔºÅ Nice to meet you‚ÄîI'm Sonoma, built by Oak AI. How can I help today?\n",
      "prompt_tokens: 35 \n",
      "completion_tokens: 24 \n",
      "total_tokens: 60\n",
      "reasoning_tokens: 1\n",
      "\n",
      " cohere.command-a-03-2025  : Hello! ‰Ω†Â•ΩÔºÅ How can I assist you today? Whether it's answering questions, helping with a task, or just chatting, I'm here for you.\n",
      "prompt_tokens: 6 \n",
      "completion_tokens: 34 \n",
      "total_tokens: 40\n",
      "\n",
      " xai.grok-3-mini  : Hello! ‰Ω†Â•ΩÔºÅ\n",
      "\n",
      "I'm Grok, an AI created by xAI to be helpful and truthful. It's great to chat‚Äîhow can I assist you today? üòä\n",
      "prompt_tokens: 12 \n",
      "completion_tokens: 35 \n",
      "total_tokens: 355\n",
      "reasoning_tokens: 308\n",
      "\n",
      " meta.llama-4-maverick-17b-128e-instruct-fp8  : Hello! ‰Ω†Â•ΩÔºÅIt's nice to meet you! Is there something I can help you with or would you like to chat? (‰Ω†Â•ΩÔºÅÂæàÈ´òÂÖ¥ËÆ§ËØÜ‰Ω†ÔºÅ‰Ω†ÈúÄË¶ÅÊàëÂ∏ÆÂä©‰ªÄ‰πàÔºåËøòÊòØÊÉ≥ËÅäÂ§©Ôºü)\n",
      "prompt_tokens: 16 \n",
      "completion_tokens: 46 \n",
      "total_tokens: 62\n",
      "\n",
      " openai.gpt-oss-20b  : Hello! ‰Ω†Â•ΩÔºÅüåü How can I help you today?  \n",
      "ÔºàÂ¶ÇÊûú‰Ω†Êõ¥ÂñúÊ¨¢Áî®‰∏≠Êñá‰∫§ÊµÅÔºåËØ∑Áõ¥Êé•ÂëäËØâÊàëÔºÅÔºâ\n",
      "prompt_tokens: 72 \n",
      "completion_tokens: 126 \n",
      "total_tokens: 198\n"
     ]
    }
   ],
   "source": [
    "for model in test_models:\n",
    "    print(\"\\n\", model, \" : \", end=\"\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"hello! ‰Ω†Â•ΩÔºÅ\"}],\n",
    "        max_tokens=1024,\n",
    "        stream=True,  # this time, we set stream=True\n",
    "    )\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "        if chunk.usage:\n",
    "            print(\n",
    "                \"\\nprompt_tokens:\",\n",
    "                chunk.usage.prompt_tokens,\n",
    "                \"\\ncompletion_tokens:\",\n",
    "                chunk.usage.completion_tokens,\n",
    "                \"\\ntotal_tokens:\",\n",
    "                chunk.usage.total_tokens,\n",
    "            )\n",
    "            if chunk.usage.completion_tokens_details:\n",
    "                print(\n",
    "                    \"reasoning_tokens:\",\n",
    "                    chunk.usage.completion_tokens_details.reasoning_tokens,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test multi-modal with image input response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBxMTEhUSExIVFRUXFxUVFhUWFRUVFRUVFRUWFhUVF\n",
      "xai.grok-4  :  The image depicts an adorable white kitten, likely a Munchkin breed (known for their short legs), standing on a paved path outdoors. The cat has fluffy, pure white fur, large blue eyes, and a slightly rounded face with a curious or surprised expression as it gazes directly at the camera. Its body is compact and plump, with stubby legs that give it a somewhat dwarfed appearance. The background features vibrant green grass with a few small yellow flowers, suggesting a sunny, natural setting like a garden or park. The overall vibe is cute and endearing, with soft lighting highlighting the kitten's fur. \n",
      "\n",
      "promptTokens:  945 \n",
      "completionTokens:  121 \n",
      "totalTokens:  1249\n"
     ]
    }
   ],
   "source": [
    "test_models = [\"xai.grok-4\"]\n",
    "\n",
    "\n",
    "def get_image():\n",
    "    import base64\n",
    "    import mimetypes\n",
    "\n",
    "    image_path = \"./test/image.jpg\"\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_str = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    url = f\"data:{mime_type};base64,{base64_str}\"\n",
    "    print(url[:100])\n",
    "    return url\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"describe this image.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": get_image()}},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "for model in test_models:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    print(\n",
    "        model,\n",
    "        \" : \",\n",
    "        completion.choices[0].message.content,\n",
    "        \"\\n\\npromptTokens: \",\n",
    "        completion.usage.prompt_tokens,\n",
    "        \"\\ncompletionTokens: \",\n",
    "        completion.usage.completion_tokens,\n",
    "        \"\\ntotalTokens: \",\n",
    "        completion.usage.total_tokens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(latitude, longitude):\n",
    "    # import requests\n",
    "    # response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    # data = response.json()\n",
    "    # return data['current']['temperature_2m']\n",
    "    return 25.0\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"latitude\": {\"type\": \"number\"},\n",
    "                    \"longitude\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"latitude\", \"longitude\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "            \"strict\": True,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# model = \"cohere.command-a-03-2025\"\n",
    "model = \"xai.grok-3-mini\"\n",
    "# model = \"xai.grok-4\"\n",
    "# model = \"meta.llama-4-maverick-17b-128e-instruct-fp8\"\n",
    "# model = \"meta.llama-3.3-70b-instruct\"\n",
    "\n",
    "# model = \"openai.gpt-oss-120b\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Use tool `get_weather`to answer: What is the weather like in New York today? the coordinates are 40.7128¬∞ N, 74.0060¬∞ W.\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response without streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call output for model -- xai.grok-3-mini\n",
      "[ChatCompletionMessageFunctionToolCall(id='call_37360171', function=Function(arguments='{\"latitude\":40.7128,\"longitude\":-74.006}', name='get_weather'), type='function')] \n",
      "\n",
      "content text: \n",
      "\n",
      "prompt_tokens: 324 \n",
      "completion_tokens: 37 \n",
      "total_tokens: 735\n",
      "reasoning_tokens: 374\n"
     ]
    }
   ],
   "source": [
    "#### Cohere and Llama model without streaming\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools, max_tokens=1024, stream=False\n",
    ")\n",
    "print(f\"Function call output for model -- {model}\")\n",
    "print(completion.choices[0].message.tool_calls, \"\\n\")\n",
    "print(\"content text:\", completion.choices[0].message.content)\n",
    "print(\n",
    "    \"\\nprompt_tokens:\",\n",
    "    completion.usage.prompt_tokens,\n",
    "    \"\\ncompletion_tokens:\",\n",
    "    completion.usage.completion_tokens,\n",
    "    \"\\ntotal_tokens:\",\n",
    "    completion.usage.total_tokens,\n",
    ")\n",
    "if completion.usage.completion_tokens_details:\n",
    "    print(\n",
    "        \"reasoning_tokens:\", completion.usage.completion_tokens_details.reasoning_tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'detail': 'Tool call with streaming is currently not supported by xai.grok-3-mini'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#### Cohere and Llama model with streaming\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this time, we set stream=True\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(model,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:    \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m   1146\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'detail': 'Tool call with streaming is currently not supported by xai.grok-3-mini'}"
     ]
    }
   ],
   "source": [
    "#### Cohere and Llama model with streaming\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    max_tokens=1024,\n",
    "    tools=tools,\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "print(model, \":\")\n",
    "for chunk in response:\n",
    "    if not chunk.choices[0].delta.tool_calls:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "    else:\n",
    "        if model.startswith(\"cohere\"):\n",
    "            print(\"\\n\")\n",
    "        print(chunk.choices[0].delta.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function result:  25.0\n",
      "\n",
      "\n",
      "0 {'role': 'user', 'content': 'Use tool `get_weather`to answer: What is the weather like in New York today? the coordinates are 40.7128¬∞ N, 74.0060¬∞ W.'}\n",
      "1 ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_37360171', function=Function(arguments='{\"latitude\":40.7128,\"longitude\":-74.006}', name='get_weather'), type='function')])\n",
      "2 {'role': 'tool', 'tool_call_id': 'call_37360171', 'content': '25.0'}\n"
     ]
    }
   ],
   "source": [
    "#### Add tool call messages to the conversation\n",
    "new_message = completion.choices[0].message\n",
    "tool_calls = new_message.tool_calls\n",
    "messages.append(new_message)\n",
    "\n",
    "# use tool to get weather\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "    result = get_weather(args[\"latitude\"], args[\"longitude\"])\n",
    "    print(\"function result: \", result)\n",
    "    # append result message\n",
    "    messages.append(\n",
    "        {\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": str(result)}\n",
    "    )\n",
    "print(\"\\n\")\n",
    "for index, each in enumerate(messages):\n",
    "    print(index, each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output for model -- xai.grok-3-mini\n",
      "The current weather in New York (based on coordinates 40.7128¬∞ N, 74.0060¬∞ W) shows a temperature of approximately 25.0¬∞C. For more detailed weather information, please provide additional clarification if needed.\n"
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results without stream\n",
    "\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=model, messages=messages, max_tokens=1024, stream=False\n",
    ")\n",
    "print(f\"\\nOutput for model -- {model}\")\n",
    "print(completion_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output for model -- xai.grok-3-mini\n",
      "The weather in New York (coordinates: 40.7128¬∞ N, 74.0060¬∞ W) today has a temperature of approximately 25.0¬∞C. For more detailed weather information, you may need to clarify or use additional tools."
     ]
    }
   ],
   "source": [
    "# Generate answer based on tool results with stream\n",
    "\n",
    "completion_2 = client.chat.completions.create(\n",
    "    model=model, messages=messages, max_tokens=1024, stream=True\n",
    ")\n",
    "print(f\"\\nOutput for model -- {model}\")\n",
    "for chunk in completion_2:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBxMTEhUSExIVFRUXFxUVFhUWFRUVFRUVFRUWFhUVF\n",
      "model: cohere.embed-v4.0\n",
      "Usage(prompt_tokens=32, total_tokens=32)\n",
      "Usage(prompt_tokens=0, total_tokens=0)\n",
      "0: an adorable white kitten with fluffy fur, standing on a pave \n",
      " [0.049316406, -0.028686523, -0..., -0.0018692017, -0.011962891]\n",
      "1: ‰∏ÄÂè™ÊØõËå∏Ëå∏ÁöÑÂèØÁà±ÁôΩËâ≤Â∞èÁå´ÔºåÁ´ôÂú®Êà∑Â§ñÈì∫Â•ΩÁöÑÂ∞èË∑Ø‰∏ä„ÄÇ \n",
      " [0.05493164, -0.025146484, -0....6, -0.026733398, -0.009338379]\n",
      "2: data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGB \n",
      " [0.005340576, -0.013305664, -0..., -0.057617188, -0.0022277832]\n"
     ]
    }
   ],
   "source": [
    "embd_model = [\n",
    "    \"cohere.embed-v4.0\",\n",
    "    #'cohere.embed-english-light-v3.0',\n",
    "    #'cohere.embed-english-v3.0',\n",
    "    #'cohere.embed-multilingual-light-v3.0',\n",
    "    #'cohere.embed-multilingual-v3.0'\n",
    "]\n",
    "input = [\n",
    "    \"an adorable white kitten with fluffy fur, standing on a paved path outdoors.\",\n",
    "    \"‰∏ÄÂè™ÊØõËå∏Ëå∏ÁöÑÂèØÁà±ÁôΩËâ≤Â∞èÁå´ÔºåÁ´ôÂú®Êà∑Â§ñÈì∫Â•ΩÁöÑÂ∞èË∑Ø‰∏ä„ÄÇ\",\n",
    "]\n",
    "image_input = get_image()\n",
    "output = []\n",
    "\n",
    "print(\"model:\", model)\n",
    "embeddings = client.embeddings.create(input=input, model=model)\n",
    "output.extend(embeddings.data)\n",
    "print(embeddings.usage)\n",
    "\n",
    "embeddings = client.embeddings.create(input=image_input, model=model)\n",
    "output.extend(embeddings.data)\n",
    "print(embeddings.usage)\n",
    "\n",
    "inputs = input.copy()\n",
    "inputs.append(image_input)\n",
    "for index in range(len(output)):\n",
    "    print(\n",
    "        f\"{index}: {inputs[index][:60]} \\n {str(output[index].embedding)[:30]}...{str(output[index].embedding)[-30:]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 0 and 1: 0.8408057652948703\n",
      "Cosine Similarity between 0 and 2: 0.5641697354487893\n",
      "Cosine Similarity between 1 and 2: 0.5293368303400612\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cosine Similarity\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    cosine_similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "for pair in [(0, 1), (0, 2), (1, 2)]:\n",
    "    similarity = calculate_cosine_similarity(\n",
    "        output[pair[0]].embedding, output[pair[1]].embedding\n",
    "    )\n",
    "    print(f\"Cosine Similarity between {pair[0]} and {pair[1]}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
